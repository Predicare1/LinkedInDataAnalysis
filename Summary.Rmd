---
title: Analysis of the Relationship between Industry and Skill with LinkedIn Data
output: html_document
---

LinkedIn is a business-oriented social network where people build their professional profiles. In this project, the correlation between an individual's industry and his skills was analyzed. The ultimate goal of this project is to build a job recommendation system based on a person's LinkedIn profile. My initial movement here is to infer a person's industry from his skills.   

## 1. Get Data
The code in [GetOnlineData.R](https://github.com/phylliswany/LinkedInDataAnalysis/blob/master/GetOnlineData.R) is used to connect to the LinkedIn API. However, LinkedIn has changed the availability of most of their API endpoints. Very limited profiles can be accessed. 

Fortunately, [a LinkedIn data set](http://www.reddit.com/r/dataisbeautiful/comments/25qjpz/how_many_employees_are_moving_between_companies_oc/chjvd0g) was published in Reddit in the format of JSON files. The code in [GetData.R](https://github.com/phylliswany/LinkedInDataAnalysis/blob/master/GetData.R) is used to download the compressed data and unzip them into JSON files. The total volume of the data is 4.29GB. 

## 2. Clean Data
The code in [CleanData.R](https://github.com/phylliswany/LinkedInDataAnalysis/blob/master/CleanData.R) will read the JSON files and format them to be dataframes. Each dataframe contains 10 columns and following are a few rows taken as an example.    
```{r, include=FALSE, cache=TRUE}
library(jsonlite)
```
```{r, echo=FALSE, cache=TRUE}
jsonData <- fromJSON("/Users/yanwang/Desktop/LinkedinData/JsonFile/a-2.json")
head(jsonData)                   
```
Inside the code, the dataset is cleaned by only selecting rows with values assigned to all the following columns, "location", "positions", "industry", "skills" and "education". Following are a few rows as an example of cleaned data. Since only "industry" and "skills" are of primary interests, other irrelevant rows are removed. "Location", "positions", and "educations" are kept for future analysis.   
```{r, echo=FALSE, cache=TRUE}
jsonData <- fromJSON("/Users/yanwang/Desktop/LinkedinData/JsonFile/a-2.json")   
field_name <- names(jsonData)
index1 <- (jsonData$location != "NULL")
index2 <- (jsonData$positions != "NULL")
index3 <- (jsonData$industry != "NULL")
index4 <- (jsonData$skills != "NULL")
index5 <- (jsonData$educations != "NULL")
index <- (index1 & index2 & index3 & index4 & index5)
selected_data <- jsonData[index, c("location", "positions", "industry", "skills", "educations")] 
head(selected_data)
```

## 3. Select Top Industries
The code in [AnalyzeIndustry.R](https://github.com/phylliswany/LinkedInDataAnalysis/blob/master/AnalyzeIndustry.R) selects the top 20 industries which are the most popular among the individuals included in the cleaned data set. Analyasis will be conducted among the top 20 industries limited by the computational resources.

```{r, include=FALSE, cache=TRUE}
library(dplyr)
library(ggplot2)
```
```{r, echo=FALSE, cache=TRUE}
file_name <- "/Users/yanwang/Desktop/LinkedinData/IndustryAnlysis/industry.Rda"
load(file_name)
num <- 20
industrytable <- arrange(industrytable, desc(Freq))
topindustry <- head(industrytable, num)
q <- qplot(x=industry, y=Freq, data=topindustry, geom="bar", stat="identity", xlab="", ylab="Number of Cases", main=paste("Top", num, "Industries"), fill=I("red"))
q + theme(axis.text.x = element_text(angle = 45, hjust=1))
```

Within expectation, the number of people working in the "Information Technology and Services" is almost 5 times as that of other industries as people in this industry are more likely to create a LinkedIn profile. Though this phenomenon can be built into the relationship model as a pior probability of an individual's "industry", we believe tha adoption rate of LinkedIn will grow, especially in those not information-related industries.

In order to create a balanced data set with an equal number of people in each industry, the code in [ExtractIndustryData.R](https://github.com/phylliswany/LinkedInDataAnalysis/blob/master/ExtractIndustryData.R) randomly samples 2000 profiles in each industry. Furthermore, the code in [SplitData.R](https://github.com/phylliswany/LinkedInDataAnalysis/blob/master/SplitData.R) splits the total 40000 data samples into a training data set and a testing data set by 50% vs. 50%.

## 4. Select Featues in "Skills" to Correlate with "Industry"
### a. Merge Top Skills in Each Industry as a Feature Vector
The code in [AnalyzeSkill.R](https://github.com/phylliswany/LinkedInDataAnalysis/blob/master/AnalyzeSkill.R) extracts the top skills in each industry in terms of the frequency a skill appears in the "skills" column among the training data set belonging to the same industry. Following shows the top 20 skills voted by the data samples in the training data set belonging to the industry of "Accounting".

```{r, echo=FALSE, cache=TRUE}
file_name <- "/Users/yanwang/Desktop/LinkedinData/IndustryAnlysis/skilltraining.Rda"
load(file_name)
industrygroup <- split(newdata, newdata$industry) # split the data into industries
rename <- dplyr::rename
num <- 20
CountSkill <- function(dataframe, n){
  skillgroup <- as.data.frame(table(factor(dataframe$skill)))
  skillgroup <- arrange(skillgroup, desc(Freq))
  topskill <- skillgroup[1:n,]
  topskill <- rename(topskill, skill=Var1)
}
topskill <- lapply(industrygroup, CountSkill, n=num)
index <- 1
industryname <- names(topskill)
industry <- industryname[index]
skilldata <- topskill[[industry]]
q <- qplot(x=skill, y=Freq, data=skilldata, geom="bar", stat="identity", xlab="", ylab="Number of Cases", main=paste("Top", num, "Skills of", industry), fill=I("red"))
q + theme(axis.text.x = element_text(angle = 45, hjust=1))
```

Inevitably, skills across the top industries overlap. One example is "Banking" vs. "Financial Services". Therefore, when the top 20 skills of each industry are aggregated into a singla feature vector, the length is 227 much shorter than 400 if no overlap occurs.
```{r, include=FALSE, cache=TRUE}
library(reshape2)
```
```{r, echo=FALSE, cache=TRUE}
options(warn=-1)
file_name <- "/Users/yanwang/Desktop/LinkedinData/IndustryAnlysis/industryfeature_20.Rda"
load(file_name)
newdata <- lapply(industryfeature, function(data) data.frame(as.list(setNames(data$Freq, data$skill))))
data <- do.call("rbind", newdata)
data1 <- as.data.frame(t(data))
data2 <- cbind(skill=industryfeature$Accounting$skill, data1)
data3 <- data2[10:20,]
data.m <- melt(data3, id.vars="skill")
ggplot(data.m, aes(x=skill , y=value, fill=variable)) + geom_bar(stat='identity') + xlab("Skills") + ylab("Number of Cases") + theme(axis.text.x = element_text(angle = 45, hjust=1))
```

Here binary values are assigned to each element of the feature vector where "1" indicates the skill is required (ranked as the top skills for the industry observed on the training data set) and "0" indicates the skill may be irrelevant. 

### b. Merge Top Keywords in Each Industry as a Feature Vector
However, unlike "industry", "skills" can be input in free styles, which leads to redundancy of the feature vector. For example, "accounting" and "financial accounting" selected as two separate features refer to pretty much the same skill. Therefore, instead of using skills, keywords are extracted from all the skills by breaking phrases into words.

The code in [AnalyzeKeyword.R](https://github.com/phylliswany/LinkedInDataAnalysis/blob/master/AnalyzeKeyword.R) selects the top keywords in each industry. Following shows the top 20 keywords appearing in the column of "skills" from the samples belonging to the industry of "Accounting" in the training data set.

```{r, echo=FALSE, cache=TRUE}
file_name <- "/Users/yanwang/Desktop/LinkedinData/IndustryAnlysis/keywordtraining.Rda"
load(file_name)
newdata <- newdata[-which(newdata$keyword == ""),] # remove the space in the keyword list
industrygroup <- split(newdata, newdata$industry) # split the data into industries
rename <- dplyr::rename
mutate <- dplyr::mutate
num <- 20
CountKeyword <- function(dataframe, n){
  skillgroup <- as.data.frame(table(factor(dataframe$keyword)))
  skillgroup <- arrange(skillgroup, desc(Freq))
  topskill <- skillgroup[1:n,]
  topskill <- rename(topskill, keyword=Var1)

}
topkeyword <- lapply(industrygroup, CountKeyword, n=num)
topkeyword1 <- topkeyword
index <- 1
industryname <- names(topkeyword1)
industry <- industryname[index]
keyworddata <- topkeyword1[[industry]]
q <- qplot(x=keyword, y=Freq, data=keyworddata, geom="bar", stat="identity", xlab="", ylab="Number of Cases", main=paste("Top", num, "Keywords of", industry), fill=I("red"))
q + theme(axis.text.x = element_text(angle = 45, hjust=1))
```

But some keywords like "management", "development" and etc. are widely used and not specific enough to correlate with any specific industries. To include this kind of words in the feature vetor is not very meaningful. So before aggregating the top 20 keywords in the feature vector, the common keywords shared by all the top 20 industries are removed.

Instead of using binary values, each keyword is weighted according to the frequency appearing in the column of "skills" within the samples belonging to the same industry in the training data set. The weight is further normalized by being divided by the sum of the frequencies of all the top keywords in the industry. Following shows part of the feature vector to model the industry of "accounting". Note that the total weight for each feature vector characterizing a specific indusry should always be 1.

```{r, echo=FALSE, cache=TRUE}
file_name <- "/Users/yanwang/Desktop/LinkedinData/IndustryAnlysis/industryfeaturekeyword_20.Rda"
load(file_name)
example <- as.data.frame(industryfeature$Accounting)
example <- example[1:20,]
q <- qplot(x=keyword, y=weight, data=example, geom="bar", stat="identity", xlab="Subset of the Keywords", ylab="Weight", main="Feature Vector of Accounting", fill=I("red"))
q + theme(axis.text.x = element_text(angle = 45, hjust=1))
```

## 5. Correlate "Industry" with "Skills"
### a. Item based Recommendation System
To build an item based recommendation system, the same feature vector that characterizes the individual industries is used to characterize a person. But binary values are assigned no matter each element in the feature vector presents skills or keywords. The cosine distance between the feature vector of the person and that of each industry is calcualted. The industry resulting in the largest cosin distance is recommended. The code in [ClassifySubject.R](https://github.com/phylliswany/LinkedInDataAnalysis/blob/master/ClassifySubject.R) tests the recommendation system on the training data set and following shows the accuracy of using either skills or keywords.

```{r, echo=FALSE, cache=TRUE}
result <- data.frame(Feature.Length=c(20, 40, 80, 160, 320, 20, 40, 80, 160, 320), Feature=c("Skill", "Skill", "Skill", "Skill", "Skill", "Keyword","Keyword", "Keyword", "Keyword", "Keyword"), Accuracy=c(0.3244, 0.3534, 0.3635, 0.3581, 0.3418, 0.4242, 0.4550, 0.4780, 0.4866, 0.4947))
ggplot(data=result, aes(x=Feature.Length, y=Accuracy, group=Feature, colour=Feature)) + geom_line() + geom_point() + xlab("Feature Length")
```

Note that the feature length refers to the number of top skills or keywords extracted from each industry. Since there are overlaps across the 20 industries, the actual length of the feature vector cannot be simply calculated by multiplying by 20. In addition, "keyword" outperforms "skill" in all the ranges of the feature length and the accuracy is still increasing with the increased number of features.

### b. Supervised Learning Algorithms  
Supervised learning algorithms are then applied to the trainining data set without modeling the individual industries beforehand. The code in [IndustryClassifier.R](https://github.com/phylliswany/LinkedInDataAnalysis/blob/master/IndustryClassifier.R) tries different supervised learning algorithms on the training data set and following shows the accuracy by testing on the training data set itself when setting the length of the feature vector to be 20.

```{r, echo=FALSE, cache=TRUE}
result <- data.frame(Algorithm=c("Tree", "Naive Bayes", "Linear SVM", "SVM with RBF Kernal"), Accuracy=c(0.2548, 0.1070, 0.5216, 0.5154))
qplot(x=Algorithm, y=Accuracy, data=result, geom="bar", stat="identity", fill=I("red"))
```

Among the four supervised learning algorithms, the linear support vector machine has the best performance. So feature vectors with a range of different lengths are fed into the algorithm for performance evaluation with the training data set.

```{r, echo=FALSE, cache=TRUE}
result <- data.frame(Feature.Length=c(20, 40, 80, 160, 320), Feature=c("Keyword","Keyword", "Keyword", "Keyword", "Keyword"), Accuracy=c(0.5216, 0.5841, 0.7054, 0.8423, 0.8878))
ggplot(data=result, aes(x=Feature.Length, y=Accuracy, group=Feature, colour=Feature)) + geom_line() + geom_point() + xlab("Feature Length")
```

## 6. Performance Evaluation with the Testing Data Set
Evidently keyword based features outperform skill based features. So only keyword based features are used to analyze the testing data set. Both item based recommendation system ([ClassifyTesting.R](https://github.com/phylliswany/LinkedInDataAnalysis/blob/master/ClassifyTesting.R)) and linear support vector machine ([TestClassifier.R](https://github.com/phylliswany/LinkedInDataAnalysis/blob/master/TestClassifier.R)) are applied to infer an individual's working industry. Following shows the result.

```{r, echo=FALSE, cache=TRUE}
result <- data.frame(Feature.Length=c(20, 40, 80, 160, 320, 20, 40, 80, 160, 320), Algorithm=c("IRS", "IRS", "IRS", "IRS", "IRS", "SVM","SVM", "SVM", "SVM", "SVM"), Accuracy=c(0.4242, 0.455, 0.478, 0.4866, 0.4947, 0.4689, 0.4728, 0.4622, 0.4336, 0.4176))
ggplot(data=result, aes(x=Feature.Length, y=Accuracy, group=Algorithm, colour=Algorithm)) + geom_line() + geom_point() + xlab("Feature Length")
```

Though linear SVM works better in the training data set, especially when increasing the dimension of the feature vector. However, it presents overfitting problem as the performance deteriorates with the testing data set. On the other hand, item based recommendation system  presents stability across both the training data set and the testing data set. Moreover, the performance increases with the increased dimension of the feature vector and is expected to perform better with a longer feature vector. One explanation here is a short feature vector may fail to include the keywords included in an individual's profile. Therefore, the longer the feature vetor, the larger the possibility that an individual's keywords will be included.  
```{r, include=FALSE, cache=TRUE}
library(caret)
```
```{r, echo=FALSE, cache=TRUE}
name_list <- c("20_0.4212", "40_0.4477", "80_0.4672", "160_0.4779", "320_0.4832")
data <- NULL
for (i in 1:length(name_list)){
  file_name <- paste("/Users/yanwang/Desktop/LinkedinData/IndustryAnlysis/resulttestingkeyword_", name_list[i], ".Rda", sep="")
  load(file_name)
  result$prediction <- droplevels(result$prediction)
  result$truth <- droplevels(result$truth)
  levels(result$truth) <- c(levels(result$truth), "Unknown")
  x <- confusionMatrix(result$prediction, result$truth)
  data <- c(data, sum(x$table[21,]))
  }
result <- data.frame(Feature.Length=c(20, 40, 80, 160, 320), Freq=data)
ggplot(data=result, aes(x=Feature.Length, y=Freq, colour="red")) + geom_line() + geom_point() + xlab("Feature Length") + guides(colour=FALSE) + ggtitle("Number of Unknown Cases")
```

However the number of unknown cases increases a little bit when the dimension of the feature vector increases from 160 to 320. It is because more common words are removed in this case. But since the accuracy is still increasing, we still like longer feature vectors.

## 7. Industry Group
The accuracy of 0.4832 of item based reconmmendation system with 320 keywords describing each industry is not satisfying. To diagnose the problem, we plot the confusion matrix of the classification result.

```{r, echo=FALSE, cache=TRUE}
options(warn=-1)
file_name <- "/Users/yanwang/Desktop/LinkedinData/IndustryAnlysis/resulttestingkeyword_320_0.4832.Rda"
load(file_name)
result$prediction <- droplevels(result$prediction)
result$truth <- droplevels(result$truth)
levels(result$truth) <- c(levels(result$truth), "Unknown")
x <- confusionMatrix(result$prediction, result$truth)
y <- as.data.frame(x$table)
ggplot(y) + geom_tile(aes(x=Reference, y=Prediction, fill=Freq)) + theme(axis.text.x = element_text(angle = 45, hjust=1))
```

Here we want all the bright colors to concentrate around the diagonal. But we observe some bright squares off the diagonal. For example, "banking"" has a relatively large proportion to be classified as "financial services". This makes a lot of sense as these two areas require quite similar skills and people from one industry will be easily switched to the other. Another example is between "computer software" and "information technology and services". In a job recommendation system, we want to give users more options. So listing all the job opportunities in the industries matching an individual's skills is actually wanted. Therefore, we want to first group the industries. The code in   
